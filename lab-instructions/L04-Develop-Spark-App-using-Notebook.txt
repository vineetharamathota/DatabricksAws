
 Lab 4: Develop Spark Application using AWS Databricks Notebook
 --------------------------------------------------------------
 ** This is continuations of Lab 3 **
 
 1. Create a workspace, launch a cluster and create a notebook
 
 2. Run the following commands to preview the data
 
	%fs ls dbfs:/FileStore/tables/
	%fs ls dbfs:/FileStore/tables/orders/
	%fs ls dbfs:/FileStore/tables/order_items/
	
 3. Run the following spark script snippets in separate cells 
 
	schema = "ord_id INT, ord_date STRING, cust_id INT, status STRING"
	orders = spark.read.csv('dbfs:/FileStore/tables/orders/', schema=schema)
	
	orders.show()
	display(orders)
	
	schema_oi = "oi_id INT, oi_ord_id INT, oi_prod_id INT, oi_quantity INT, oi_subtotal FLOAT, oi_prod_price FLOAT"
	order_items = spark.read.csv('dbfs:/FileStore/tables/order_items/', schema=schema_oi)
	
	from pyspark.sql.functions import sum, round
	
	revenue_by_day = orders.filter('status IN ("CLOSED", "COMPLETE")') \
    .join(order_items, orders.ord_id == order_items.oi_ord_id ) \
    .groupBy(orders.ord_date) \
    .agg( round(sum(order_items.oi_subtotal), 2).alias('daily_revenue')) \
    .orderBy(orders.ord_date)
		
	revenue_by_day.coalesce(1).write.csv("dbfs:/FileStore/tables/output/daily_revenue", header=True)

