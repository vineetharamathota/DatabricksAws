
 Lab 20: Incremental data processing using Structured Streaming
 ---------------------------------------------------------------
 
 1. Launch databricks UI from a databricks workspace.
  
 2. Add an instance profile to the UI with a role that has AmazonS3FullAcess policy attached.
    (Refer to L07-Setup-AWS-resources-for-Databricks.txt for detailed steps)
 
	Instance Profile: CTSDatabaricksEC2S3FullAccessRole
 
 3. Create an All-purpose compute cluster and attach instance profile to the cluster.

	3.1 Create a single node cluster with the following details:

		Name: CTS Demo Cluster  (Edit the heading by clicking on pencil icon)
		Policy: Unrestricted - Single Node
		Use photon acceleration: Uncheck
		Node type: m5d.large (8 GB Memory, 2 Cores)
		Terminate after 15 minutes of inactivity
		Instance profile: CTSDatabaricksEC2S3FullAccessRole

		* Leave all other options as defaults
		
	3.2 Click on 'Create compute' button

 4. Import the notebook 'L20-Incremental-Loads-using-Structured-Streaming.dbc' and run the cells. 

	In this notebook, we demostrate the following:

	* Setup required S3 buckets to process the data from.
	* Download ‘GitHub Archive’  files and save them in S3 bucket (landing zone)
	* Process the data in the landing zone and write to the bronze layer as Delta files
	* Download additional files into the landing zone
	* Process the (new) data (only) and write to the bronze layer
	* Understand how the incremental processing works

 
 