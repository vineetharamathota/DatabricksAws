
 Lab 22: Incremental data processing with Auto Loader using File Notifications
 ------------------------------------------------------------------------------
 NOTE: This is continuation of previous lab
 
 
 1. Launch databricks UI from a databricks workspace.
 
 2. Add an instance profile to the UI with a role that has AmazonS3FullAcess policy attached.
    (Refer to L07-Setup-AWS-resources-for-Databricks.txt for detailed steps)
 
	Instance Profile: CTSDatabaricksEC2S3FullAccessRole
	
 3. Open the instance profile role (CTSDatabaricksEC2S3FullAccessRole) from IAM console
    To this role attach a new inline policy as follows:
	
		Name: CTSDatabaricksAutoLoaderFileNotifications
		JSON Content: As shown towards the end of this document
 
 4. Create an All-purpose compute cluster and attach instance profile to the cluster.

	4.1 Create a single node cluster with the following details:

		Name: CTS Demo Cluster  (Edit the heading by clicking on pencil icon)
		Policy: Unrestricted - Single Node
		Use photon acceleration: Uncheck
		Node type: m5d.large (8 GB Memory, 2 Cores)
		Terminate after 15 minutes of inactivity
		Instance profile: CTSDatabaricksEC2S3FullAccessRole

		* Leave all other options as defaults
		
	4.2 Click on 'Create compute' button

 5. Import the notebook 'L22-AutoLoader-File-Notifications.dbc' and run the cells. 

	In this notebook, we demostrate the following:

	* Setup required S3 buckets to process the data from.
	* Download ‘GitHub Archive’  files and save them in S3 bucket (landing zone)
	* Process the data in the landing zone and write to the bronze layer as Delta files using cloudFiles
	* Add additional files into the landing zone
	* Process the (new) data (only) and write to the bronze layer using cloudFiles
	* Understand how the incremental processing works

 
 
 
 
 
 ===========================
 Policy Name: CTSDatabaricksAutoLoaderFileNotifications
 
 Policy JSON:
 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "DatabricksAutoLoaderSetup",
            "Effect": "Allow",
            "Action": [
                "s3:GetBucketNotification",
                "s3:PutBucketNotification",
                "s3:ListBucket",
                "s3:GetObject",
                "sns:ListSubscriptionsByTopic",
                "sns:GetTopicAttributes",
                "sns:SetTopicAttributes",
                "sns:CreateTopic",
                "sns:TagResource",
                "sns:Publish",
                "sns:Subscribe",
                "sqs:CreateQueue",
                "sqs:DeleteMessage",
                "sqs:DeleteMessageBatch",
                "sqs:ReceiveMessage",
                "sqs:SendMessage",
                "sqs:GetQueueUrl",
                "sqs:GetQueueAttributes",
                "sqs:SetQueueAttributes",
                "sqs:TagQueue",
                "sqs:ChangeMessageVisibility",
                "sqs:ChangeMessageVisibilityBatch"
            ],
            "Resource": [
                "arn:aws:s3:::*",
                "arn:aws:sqs:*:*:databricks-auto-ingest-*",
                "arn:aws:sns:*:*:databricks-auto-ingest-*"
            ]
        },
        {
            "Sid": "DatabricksAutoLoaderList",
            "Effect": "Allow",
            "Action": [
                "sqs:ListQueues",
                "sqs:ListQueueTags",
                "sns:ListTopics"
            ],
            "Resource": "*"
        },
        {
            "Sid": "DatabricksAutoLoaderTeardown",
            "Effect": "Allow",
            "Action": [
                "sns:Unsubscribe",
                "sns:DeleteTopic",
                "sqs:DeleteQueue"
            ],
            "Resource": [
                "arn:aws:sqs:*:*:databricks-auto-ingest-*",
                "arn:aws:sns:*:*:databricks-auto-ingest-*"
            ]
        }
    ]
}

 
 